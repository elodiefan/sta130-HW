{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85fc69e6",
   "metadata": {},
   "source": [
    "# Pre-Lecture HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb401ee9",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32475a31",
   "metadata": {},
   "source": [
    "**1. Difference between Simple Linear Regression and Multiple Linear Regression**\n",
    "- Simple Linear Regression models the relationship between a single predictor (independent variable) and a single outcome (dependent variable) by fitting a straight line to the data. Multiple Linear Regression extends this by using multiple predictors to model the outcome.\n",
    "- **Benefit of Multiple Linear Regression**: It allows the model to account for the effects of multiple factors simultaneously, improving prediction accuracy and better capturing the complexity of real-world relationships.\n",
    "\n",
    "**2. Continuous vs. Indicator Variables in Simple Linear Regression**\n",
    "- A **continuous variable** can take any numeric value within a range (e.g., temperature, weight) and is used in regression to predict variations in the outcome along a smooth, continuous line.\n",
    "- An **indicator variable** (or dummy variable) represents categories, typically taking values like 0 or 1 (e.g., gender as male/female). In regression, it indicates whether a certain condition is met.\n",
    "- **Two Linear Forms**: Using a continuous predictor creates a single, smooth regression line, while using an indicator variable creates distinct levels or \"jumps\" in the predicted outcome, distinguishing between groups.\n",
    "\n",
    "**3. Effect of Adding a Single Indicator Variable in Multiple Linear Regression**\n",
    "- When both a continuous variable and an indicator variable are used together, the model adjusts the intercept based on the indicator’s value, while the continuous variable’s coefficient affects the slope.\n",
    "- **Two Linear Forms**: In Simple Linear Regression, the continuous variable alone would produce a single line. Adding an indicator in Multiple Linear Regression leads to two parallel lines (one for each level of the indicator), each line shifted vertically based on the indicator’s level.\n",
    "\n",
    "**4. Effect of Adding an Interaction Term between a Continuous and Indicator Variable**\n",
    "- Adding an interaction term (the product of the continuous and indicator variables) allows the slope to differ between groups defined by the indicator variable, so the relationship between the continuous predictor and outcome changes depending on the indicator’s level.\n",
    "- **Linear Form**: This model produces two lines with potentially different slopes, providing more flexibility to capture varying relationships across groups.\n",
    "\n",
    "**5. Behavior of a Multiple Linear Regression Model with Indicator Variables for a Non-Binary Categorical Variable**\n",
    "- If the model includes only indicator variables derived from a categorical variable with multiple levels, it represents each category as a separate group effect on the outcome. The resulting model captures the distinct impact of each category, with no slope for any continuous predictor.\n",
    "- **Linear Form and Encoding**: The model requires binary encoding (e.g., dummy coding) for each category except a reference category, resulting in a form where each indicator’s coefficient represents the mean difference between its category and the reference category. This type of encoding facilitates interpretation of category-specific effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54052470",
   "metadata": {},
   "source": [
    "**[Link to the ChatBot Session](https://chatgpt.com/share/672e6450-3ca4-800e-8d2f-32d9cf9d2fde)**\n",
    "\n",
    "**Summary of the ChatBot Session**\n",
    "In this session, we discussed several key concepts in linear regression. First, I explained the difference between Simple Linear Regression and Multiple Linear Regression. Simple Linear Regression involves a single predictor, while Multiple Linear Regression incorporates multiple predictors, which improves prediction accuracy and better reflects complex relationships.\n",
    "\n",
    "Then, I outlined the roles of continuous and indicator variables in Simple Linear Regression. Continuous variables provide a smooth line of prediction, while indicator variables introduce distinct group effects, resulting in jumps between categories. We also explored the impact of adding an indicator variable alongside a continuous variable in Multiple Linear Regression, which creates two parallel lines, each representing a different level of the indicator.\n",
    "\n",
    "Next, I described the effect of adding an interaction term between a continuous and an indicator variable. This addition allows for non-parallel lines, meaning the relationship’s slope changes depending on the indicator's level. Lastly, I explained a Multiple Linear Regression model that only uses indicator variables for a non-binary categorical variable, requiring binary encoding (e.g., dummy variables) for each category. This model captures category-specific effects with each coefficient indicating the mean difference relative to a reference category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dce89b",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6489ef",
   "metadata": {},
   "source": [
    "### (1) Explain how to use these two formulas to make predictions of the outcome, and give a high level explaination in general terms of the difference between predictions from the models with and without the interaction\n",
    "\n",
    "1. In this scenario:\n",
    "\n",
    "- **Outcome Variable (Dependent Variable)**: The outcome we want to predict is **sales of sports equipment**, which depends on the advertising campaigns.\n",
    "- **Predictor Variables (Independent Variables)**: The predictor variables are:\n",
    "    - **TV advertising budget** (amount spent on TV ads, potentially continuous).\n",
    "    - **Online advertising budget** (amount spent on online ads, potentially continuous).\n",
    "\n",
    "Given the company’s setup, the effectiveness of one advertising channel may influence the other, creating an **interaction effect**. This means that the effect of TV advertising on sales could change depending on the online ad budget, and vice versa.\n",
    "\n",
    "2. **Without Interaction Effect**\n",
    "\n",
    "The linear model without interaction is:\n",
    "\n",
    "Sales = $\\beta_0$ + $\\beta_1$ (TV) + $\\beta_2$ (Online) + $\\epsilon$\n",
    "\n",
    "In this model: \n",
    "- $\\beta_0$: intercept term\n",
    "- $\\beta_1$: Effect of TV advertising on sales.\n",
    "- $\\beta_2$: Effect of online advertising on sales.\n",
    "- $\\epsilon$: Error term, accounting for variability not explained by the model.\n",
    "\n",
    "This model assumes that the effects of TV and online ads on sales are **independent** of each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ebeff",
   "metadata": {},
   "source": [
    "3. **With Interaction Effect**\n",
    "\n",
    "The linear model with interaction is:\n",
    "\n",
    "Sales = $\\beta_0$ + $\\beta_1$ (TV) + $\\beta_2$ (Online) + $\\beta_3$ * (TV * Online) + $\\epsilon$\n",
    "\n",
    "Here, $\\beta_3$ represents the **interaction effect** between TV and online ads. If significant, this term implies that the effectiveness of TV ads on sales is different depending on the online ad spending level and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b85b0",
   "metadata": {},
   "source": [
    "**Predicting Sales with Both Models**\n",
    "\n",
    "- **Without Interaction**: To predict sales without the interaction, we substitute values for **TV** and **Online** into the first formula. The predicted sales will be based on the separate, additive effects of TV and online ads. In this model, increasing spending on TV ads will always increase sales by a fixed amount per dollar, regardless of online ad spending.\n",
    "\n",
    "- **With Interaction**: For the interaction model, predictions are made by substituting **TV, Online**, and their product (**TV** × **Online**) into the second formula. This prediction accounts for how the effectiveness of one ad type changes based on spending in the other. The model implies that increasing TV ad spend has a different effect on sales depending on how much is spent on online ads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b9a97",
   "metadata": {},
   "source": [
    "### (2) Explain how to update and use the implied two formulas to make predictions of the outcome if, rather than considering two continuous predictor variables, we instead suppose the advertisement budgets are simply categorized as either \"high\" or \"low\" (binary variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823351c",
   "metadata": {},
   "source": [
    "**High-Level Explanation of Model Differences**\n",
    "- **Without Interaction**: Predicts the impact of TV and online ads independently, meaning spending more on one type of ad always has the same impact on sales.\n",
    "- **With Interaction**: Predicts that the effect of TV ads depends on online ad spending, and vice versa. It suggests that a coordinated ad strategy (spending more on both) might yield different results than spending on one type only.\n",
    "\n",
    "**Modifying the Models for Categorical Budgets (High vs. Low)**\n",
    "Suppose the ad budgets are classified as **High** or **Low**:\n",
    "- Let **TV** be 1 if high-budget and 0 if low-budget.\n",
    "- Let **Online** be 1 if high-budget and 0 if low-budget.\n",
    "\n",
    "**1. Without Interaction (Categorical)**\n",
    "Sales = $\\beta_0$ + $\\beta_1$ (TV) + $\\beta_2$ (Online) + $\\epsilon$\n",
    "\n",
    "Here:\n",
    "- $\\beta_1$ represents the impact on sales of having a high TV budget.\n",
    "- $\\beta_2$represents the impact on sales of having a high online budget.\n",
    "\n",
    "**2. With Interaction (Categorical)**\n",
    "\n",
    "Sales = $\\beta_0$ + $\\beta_1$ (TV) + $\\beta_2$ (Online) + $\\beta_3$ * (TV * Online) + $\\epsilon$\n",
    "\n",
    "Here, $\\beta_3$ now represents the additional effect on sales when both TV and online ad budgets are high.\n",
    "\n",
    "**Using the Updated Models**\n",
    "- **Without Interaction**: Predict sales based on whether each budget is high or low, assuming the effects are independent.\n",
    "- **With Interaction**: The prediction adjusts based on the combined effect of high budgets in both categories, allowing for the possibility that spending more in one category enhances or diminishes the effect of the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aa0c39",
   "metadata": {},
   "source": [
    "**[Link to the ChatBot Session](https://chatgpt.com/share/672e7575-6514-800e-9794-b0cadd3d61eb)**\n",
    "\n",
    "**Summary of the ChatBot Session**\n",
    "\n",
    "Here's a summary of our interactions in this session:\n",
    "\n",
    "You provided a scenario involving a company that sells sports equipment, which runs advertising campaigns on TV and online platforms. You asked for help in identifying outcome and predictor variables, discussing potential interaction effects, and formulating linear models with and without interactions. You also requested explanations on using the models for predictions, including the differences between predictions with and without interaction effects.\n",
    "\n",
    "I identified sales as the outcome variable and TV and online advertising budgets as the predictor variables. I explained that an interaction effect might be present, where the effectiveness of one advertising medium depends on the budget allocated to the other.\n",
    "\n",
    "I provided two linear models:\n",
    "\n",
    "Without Interaction: A simple additive model where the effects of TV and online ads on sales are independent.\n",
    "With Interaction: A model including an interaction term to account for the combined effect of TV and online budgets.\n",
    "I described how to make predictions using both models and explained, at a high level, the differences between the predictions. The model with interaction implies a coordinated ad strategy where the effect of one type of ad depends on the spending level of the other.\n",
    "\n",
    "Lastly, I discussed how to adjust the models if the ad budgets were categorized as \"high\" or \"low\" (binary variables), showing how to update the formulas for categorical predictors and interpret predictions in this modified context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15487d",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dd6fcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.228109\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>str8fyre</td>     <th>  No. Observations:  </th>  <td>   800</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   788</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    11</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 13 Nov 2024</td> <th>  Pseudo R-squ.:     </th>  <td>0.05156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>04:13:26</td>     <th>  Log-Likelihood:    </th> <td> -182.49</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -192.41</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th>  <td>0.04757</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                      <td></td>                        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                <td>   -3.2644</td> <td>    0.714</td> <td>   -4.572</td> <td> 0.000</td> <td>   -4.664</td> <td>   -1.865</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                        <td>    4.3478</td> <td>    2.179</td> <td>    1.996</td> <td> 0.046</td> <td>    0.078</td> <td>    8.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 2\") == \"None\")[T.True]</th>         <td>    1.5432</td> <td>    0.853</td> <td>    1.810</td> <td> 0.070</td> <td>   -0.128</td> <td>    3.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>                       <td>   -0.0574</td> <td>    0.468</td> <td>   -0.123</td> <td> 0.902</td> <td>   -0.975</td> <td>    0.861</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>                       <td>   -0.6480</td> <td>    0.466</td> <td>   -1.390</td> <td> 0.164</td> <td>   -1.561</td> <td>    0.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>                       <td>   -0.8255</td> <td>    0.545</td> <td>   -1.516</td> <td> 0.130</td> <td>   -1.893</td> <td>    0.242</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>                       <td>   -0.5375</td> <td>    0.449</td> <td>   -1.198</td> <td> 0.231</td> <td>   -1.417</td> <td>    0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>                       <td>    0.3213</td> <td>    0.477</td> <td>    0.673</td> <td> 0.501</td> <td>   -0.614</td> <td>    1.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                   <td>    0.0172</td> <td>    0.006</td> <td>    3.086</td> <td> 0.002</td> <td>    0.006</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                 <td>   -0.0365</td> <td>    0.019</td> <td>   -1.884</td> <td> 0.060</td> <td>   -0.074</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                  <td>   -0.0098</td> <td>    0.008</td> <td>   -1.247</td> <td> 0.213</td> <td>   -0.025</td> <td>    0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:I(Q(\"Type 2\") == \"None\")[T.True]</th> <td>   -0.0197</td> <td>    0.012</td> <td>   -1.651</td> <td> 0.099</td> <td>   -0.043</td> <td>    0.004</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                           &     str8fyre     & \\textbf{  No. Observations:  } &      800    \\\\\n",
       "\\textbf{Model:}                                   &      Logit       & \\textbf{  Df Residuals:      } &      788    \\\\\n",
       "\\textbf{Method:}                                  &       MLE        & \\textbf{  Df Model:          } &       11    \\\\\n",
       "\\textbf{Date:}                                    & Wed, 13 Nov 2024 & \\textbf{  Pseudo R-squ.:     } &  0.05156    \\\\\n",
       "\\textbf{Time:}                                    &     04:13:26     & \\textbf{  Log-Likelihood:    } &   -182.49   \\\\\n",
       "\\textbf{converged:}                               &       True       & \\textbf{  LL-Null:           } &   -192.41   \\\\\n",
       "\\textbf{Covariance Type:}                         &    nonrobust     & \\textbf{  LLR p-value:       } &  0.04757    \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                  & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                &      -3.2644  &        0.714     &    -4.572  &         0.000        &       -4.664    &       -1.865     \\\\\n",
       "\\textbf{Legendary[T.True]}                        &       4.3478  &        2.179     &     1.996  &         0.046        &        0.078    &        8.618     \\\\\n",
       "\\textbf{I(Q(\"Type 2\") == \"None\")[T.True]}         &       1.5432  &        0.853     &     1.810  &         0.070        &       -0.128    &        3.215     \\\\\n",
       "\\textbf{C(Generation)[T.2]}                       &      -0.0574  &        0.468     &    -0.123  &         0.902        &       -0.975    &        0.861     \\\\\n",
       "\\textbf{C(Generation)[T.3]}                       &      -0.6480  &        0.466     &    -1.390  &         0.164        &       -1.561    &        0.265     \\\\\n",
       "\\textbf{C(Generation)[T.4]}                       &      -0.8255  &        0.545     &    -1.516  &         0.130        &       -1.893    &        0.242     \\\\\n",
       "\\textbf{C(Generation)[T.5]}                       &      -0.5375  &        0.449     &    -1.198  &         0.231        &       -1.417    &        0.342     \\\\\n",
       "\\textbf{C(Generation)[T.6]}                       &       0.3213  &        0.477     &     0.673  &         0.501        &       -0.614    &        1.257     \\\\\n",
       "\\textbf{Attack}                                   &       0.0172  &        0.006     &     3.086  &         0.002        &        0.006    &        0.028     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                 &      -0.0365  &        0.019     &    -1.884  &         0.060        &       -0.074    &        0.001     \\\\\n",
       "\\textbf{Defense}                                  &      -0.0098  &        0.008     &    -1.247  &         0.213        &       -0.025    &        0.006     \\\\\n",
       "\\textbf{Defense:I(Q(\"Type 2\") == \"None\")[T.True]} &      -0.0197  &        0.012     &    -1.651  &         0.099        &       -0.043    &        0.004     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               str8fyre   No. Observations:                  800\n",
       "Model:                          Logit   Df Residuals:                      788\n",
       "Method:                           MLE   Df Model:                           11\n",
       "Date:                Wed, 13 Nov 2024   Pseudo R-squ.:                 0.05156\n",
       "Time:                        04:13:26   Log-Likelihood:                -182.49\n",
       "converged:                       True   LL-Null:                       -192.41\n",
       "Covariance Type:            nonrobust   LLR p-value:                   0.04757\n",
       "============================================================================================================\n",
       "                                               coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                   -3.2644      0.714     -4.572      0.000      -4.664      -1.865\n",
       "Legendary[T.True]                            4.3478      2.179      1.996      0.046       0.078       8.618\n",
       "I(Q(\"Type 2\") == \"None\")[T.True]             1.5432      0.853      1.810      0.070      -0.128       3.215\n",
       "C(Generation)[T.2]                          -0.0574      0.468     -0.123      0.902      -0.975       0.861\n",
       "C(Generation)[T.3]                          -0.6480      0.466     -1.390      0.164      -1.561       0.265\n",
       "C(Generation)[T.4]                          -0.8255      0.545     -1.516      0.130      -1.893       0.242\n",
       "C(Generation)[T.5]                          -0.5375      0.449     -1.198      0.231      -1.417       0.342\n",
       "C(Generation)[T.6]                           0.3213      0.477      0.673      0.501      -0.614       1.257\n",
       "Attack                                       0.0172      0.006      3.086      0.002       0.006       0.028\n",
       "Attack:Legendary[T.True]                    -0.0365      0.019     -1.884      0.060      -0.074       0.001\n",
       "Defense                                     -0.0098      0.008     -1.247      0.213      -0.025       0.006\n",
       "Defense:I(Q(\"Type 2\") == \"None\")[T.True]    -0.0197      0.012     -1.651      0.099      -0.043       0.004\n",
       "============================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an example of how you can do this\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "pokeaman = pd.read_csv(url).fillna('None')\n",
    "\n",
    "pokeaman['str8fyre'] = (pokeaman['Type 1']=='Fire').astype(int)\n",
    "linear_model_specification_formula = \\\n",
    "'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(linear_model_specification_formula, data=pokeaman).fit()\n",
    "log_reg_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed3690",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1607f7c",
   "metadata": {},
   "source": [
    "To perform logistic regression on a dataset, follow these steps, assuming you're working with a dataset from the Canadian Social Connection Survey and need to handle categorical or binary outcomes:\n",
    "\n",
    "### 1. Preparing the Data:\n",
    "First, ensure that your categorical variable is transformed into a binary outcome if it's not already binary. If it's a multi-class categorical variable, you'll need to convert it into a binary outcome using one of the methods like one-hot encoding or creating a specific binary outcome. For example, if you're predicting whether someone is socially connected or not (with \"yes\" or \"no\" responses), you could convert it into a binary variable.\n",
    "\n",
    "### 2. Logistic Regression Model:\n",
    "Next, you’ll specify a logistic regression model using `statsmodels.formula.api`'s `logit` function. This is typically done with a formula interface, where you specify your dependent (outcome) variable and the independent (predictor) variables.\n",
    "\n",
    "Here’s an example of logistic regression where:\n",
    "- You convert a categorical variable into a binary outcome.\n",
    "- Use continuous, binary, and categorical predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65972ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.228109\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               str8fyre   No. Observations:                  800\n",
      "Model:                          Logit   Df Residuals:                      788\n",
      "Method:                           MLE   Df Model:                           11\n",
      "Date:                Wed, 13 Nov 2024   Pseudo R-squ.:                 0.05156\n",
      "Time:                        04:13:30   Log-Likelihood:                -182.49\n",
      "converged:                       True   LL-Null:                       -192.41\n",
      "Covariance Type:            nonrobust   LLR p-value:                   0.04757\n",
      "============================================================================================================\n",
      "                                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                   -3.2644      0.714     -4.572      0.000      -4.664      -1.865\n",
      "Legendary[T.True]                            4.3478      2.179      1.996      0.046       0.078       8.618\n",
      "I(Q(\"Type 2\") == \"None\")[T.True]             1.5432      0.853      1.810      0.070      -0.128       3.215\n",
      "C(Generation)[T.2]                          -0.0574      0.468     -0.123      0.902      -0.975       0.861\n",
      "C(Generation)[T.3]                          -0.6480      0.466     -1.390      0.164      -1.561       0.265\n",
      "C(Generation)[T.4]                          -0.8255      0.545     -1.516      0.130      -1.893       0.242\n",
      "C(Generation)[T.5]                          -0.5375      0.449     -1.198      0.231      -1.417       0.342\n",
      "C(Generation)[T.6]                           0.3213      0.477      0.673      0.501      -0.614       1.257\n",
      "Attack                                       0.0172      0.006      3.086      0.002       0.006       0.028\n",
      "Attack:Legendary[T.True]                    -0.0365      0.019     -1.884      0.060      -0.074       0.001\n",
      "Defense                                     -0.0098      0.008     -1.247      0.213      -0.025       0.006\n",
      "Defense:I(Q(\"Type 2\") == \"None\")[T.True]    -0.0197      0.012     -1.651      0.099      -0.043       0.004\n",
      "============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Example dataset (replace with your own dataset)\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "df = pd.read_csv(url).fillna('None')\n",
    "\n",
    "# Convert a categorical column into a binary outcome\n",
    "df['str8fyre'] = (df['Type 1'] == 'Fire').astype(int)\n",
    "\n",
    "# Fit a logistic regression model\n",
    "formula = 'str8fyre ~ Attack*Legendary + Defense*I(Q(\"Type 2\")==\"None\") + C(Generation)'\n",
    "log_reg_fit = smf.logit(formula, data=df).fit()\n",
    "\n",
    "# View the model summary\n",
    "print(log_reg_fit.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4340a0a0",
   "metadata": {},
   "source": [
    "### Explanation of the Code:\n",
    "**1. Data Loading**: We load the data using `pandas.read_csv`. Replace the `url` with the path to your actual dataset.\n",
    "\n",
    "**2. Binary Outcome Variable**: The example creates a binary outcome (`str8fyre`), where \"1\" indicates the \"Fire\" type and \"0\" indicates not \"Fire\". This is achieved using `astype(int)` to convert the boolean result of the condition into an integer.\n",
    "\n",
    "**3. Model Formula**:\n",
    "- `Attack*Legendary`: Interaction term between the `Attack` variable and the `Legendary` variable.\n",
    "- `Defense*I(Q(\"Type 2\")==\"None\")`: Interaction term between `Defense` and a binary condition where `Type 2` is \"None\".\n",
    "- `C(Generation)`: Treats `Generation` as a categorical variable.\n",
    "\n",
    "**4. Logistic Regression Fit**: `smf.logit()` is used to fit the logistic regression model, and `.fit()` computes the model.\n",
    "\n",
    "**5. Summary**: `log_reg_fit.summary()` gives a detailed output of the regression results, including coefficients, p-values, and other statistical metrics.\n",
    "\n",
    "### 3. Adapt to Your Data:\n",
    "Make sure to adjust the variable names and the formula to fit your dataset. If you're working with binary outcomes, like \"socially connected\" vs. \"not connected,\" modify the binary transformation part accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6149a5b8",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaffeaf",
   "metadata": {},
   "source": [
    "### 1. Preparing the Data:\n",
    "First, let's assume you have a dataset where:\n",
    "\n",
    "- The outcome variable is binary (e.g., 0 or 1, \"Yes\" or \"No\").\n",
    "- The predictors could be continuous (e.g., age, income) or binary categorical variables (e.g., gender, whether the person has a certain characteristic).\n",
    "\n",
    "If your categorical variables have more than two categories, you can convert them into binary variables (e.g., using one-hot encoding or binary encoding as discussed earlier).\n",
    "\n",
    "### 2. Logistic Regression Model:\n",
    "Here’s an example of how you might set up the logistic regression model using `statsmodels.formula.api` with continuous and binary predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcdd26c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.238601\n",
      "         Iterations 7\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:               str8fyre   No. Observations:                  800\n",
      "Model:                          Logit   Df Residuals:                      796\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Wed, 13 Nov 2024   Pseudo R-squ.:                0.007933\n",
      "Time:                        04:13:35   Log-Likelihood:                -190.88\n",
      "converged:                       True   LL-Null:                       -192.41\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.3836\n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept            -3.2341      0.409     -7.910      0.000      -4.035      -2.433\n",
      "Legendary[T.True]     0.0143      0.527      0.027      0.978      -1.018       1.047\n",
      "Attack                0.0058      0.005      1.275      0.202      -0.003       0.015\n",
      "Generation_1          0.3891      0.328      1.187      0.235      -0.253       1.032\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Example dataset (replace with your own dataset)\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "df = pd.read_csv(url).fillna('None')\n",
    "\n",
    "# Convert a categorical column into a binary outcome\n",
    "df['str8fyre'] = (df['Type 1'] == 'Fire').astype(int)\n",
    "\n",
    "# Let's assume 'Attack' is continuous, 'Legendary' is binary, and 'Generation' is categorical\n",
    "# We convert 'Generation' to a binary variable using one-hot encoding for simplicity\n",
    "df['Generation_1'] = (df['Generation'] == 1).astype(int)  # Convert 'Generation' to binary (e.g., 1 or not 1)\n",
    "\n",
    "# Logistic regression formula with continuous and binary predictors\n",
    "formula = 'str8fyre ~ Attack + Legendary + Generation_1'\n",
    "\n",
    "# Fit logistic regression model\n",
    "log_reg_fit = smf.logit(formula, data=df).fit()\n",
    "\n",
    "# View the model summary\n",
    "print(log_reg_fit.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6df91",
   "metadata": {},
   "source": [
    "### Explanation of the Code:\n",
    "**1. Data Loading**: Replace the dataset URL with the path to your dataset. This code loads the data and handles missing values by filling them with `'None'`.\n",
    "\n",
    "**2. Binary Outcome Variable**: The outcome (`str8fyre`) is a binary variable based on whether the Pokémon's primary type (`Type 1`) is \"Fire\" or not. The variable is set to 1 for \"Fire\" and 0 otherwise.\n",
    "\n",
    "**3. Continuous and Binary Predictors**:\n",
    "- `Attack`: A continuous variable representing the attack value of each Pokémon.\n",
    "- `Legendary`: A binary categorical variable that indicates whether a Pokémon is legendary (1 for legendary, 0 otherwise).\n",
    "- `Generation`: A categorical variable representing the Pokémon generation. We create a binary variable (`Generation_1`) to represent whether the Pokémon belongs to Generation 1 (`1`) or not (`0`).\n",
    "\n",
    "**4. Logistic Regression Formula**: The formula specifies the logistic regression model where:\n",
    "\n",
    "- `str8fyre` is the outcome variable.\n",
    "- `Attack`, `Legendary`, and `Generation_1` are the predictor variables.\n",
    "\n",
    "**5. Model Fit**: The model is fitted using `smf.logit()` and `.fit()`, and then the model summary is displayed.\n",
    "\n",
    "### 3. Adapt to Your Dataset:\n",
    "Make sure to:\n",
    "\n",
    "- Replace the dataset and variable names with those relevant to your dataset (e.g., if predicting social connection, replace `str8fyre` with your outcome variable like `social_connected`).\n",
    "- For any categorical variables with more than two categories, use the approach of turning them into binary variables (such as one-hot encoding) as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4ef5b",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a46b0e",
   "metadata": {},
   "source": [
    "### Step-by-Step Interpretation of Logistic Regression (As If It's a Linear Regression):\n",
    "\n",
    "### 1. Logistic Regression Coefficients:\n",
    "In logistic regression, the outcome variable is modeled as a logit function (log odds), not as a continuous value. However, for simplicity, we’ll treat the coefficients as if they were from a linear regression model. Here’s the breakdown:\n",
    "\n",
    "The formula for logistic regression is:\n",
    "$$\n",
    "\\text{logit}(p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
    "$$\n",
    "\n",
    "where: \n",
    "\n",
    "- $\\text{logit}(p)$ is the log odds of the probability of the outcome occurring (for example, the probability of being socially connected).\n",
    "- $\\beta_0$ is the intercept. \n",
    "- $\\beta_0$, $\\beta_2$, ..., $\\beta_n$ are the coefficients of the predictor variables $X_1$, $X_2$, ..., $X_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd149e2b",
   "metadata": {},
   "source": [
    "### 2. Interpretation of Coefficients:\n",
    "While logistic regression coefficients technically represent the change in the log odds of the outcome for a one-unit change in the predictor, we can simplify the interpretation as follows, assuming a linear interpretation:\n",
    "\n",
    "- **Intercept**: The intercept $\\beta_0$ represents the baseline log odds of the outcome when all predictor variables are zero.\n",
    "- **Coefficients**: Each coefficient $\\beta_i$ represents the change in the log odds of the outcome for a one-unit increase in the corresponding predictor $X_i$ If we treat it as if we were interpreting linear regression, we can say:\n",
    "    - **Continuous Variables**: For every one-unit increase in a continuous predictor (e.g., `Attack`), the log odds of the outcome increase by $\\beta_i$ or equivalently, the probability of the outcome increases.\n",
    "    - **Binary Variables**: For binary variables (e.g., `Legendary`), the coefficient $\\beta_i$ tells us how the log odds of the outcome change when the variable shifts from 0 to 1 (e.g., from \"not legendary\" to \"legendary\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c965c",
   "metadata": {},
   "source": [
    "### 3. Making Predictions:\n",
    "Once the model is trained, we can use it to make predictions about the probability of the outcome occurring. The formula for making predictions is:\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n)}}\n",
    "$$\n",
    "\n",
    "But for simplicity, we’ll just focus on interpreting the prediction as if it were linear regression.\n",
    "\n",
    "- **Linear Form**: Treat the predicted log odds as a linear combination of the predictors.\n",
    "- **Prediction**: After calculating the log odds, if you wanted to predict the probability of the outcome (e.g., probability of being socially connected), you can use the logistic function to transform it into a probability value between 0 and 1.\n",
    "\n",
    "However, if you're interpreting **as a linear model**, the **raw log odds** (from the linear combination of predictors) give you an approximation of how likely the outcome is for different values of the predictors.\n",
    "\n",
    "### Example with Simplified Interpretation:\n",
    "Let’s say you fit the following logistic regression model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646fcac",
   "metadata": {},
   "source": [
    "\\text{logit}(p) = -3.5 + 0.1 \\times \\text{Attack} + 1.2 \\times \\text{Legendary} + 0.5 \\times \\text{Generation_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227e7ae",
   "metadata": {},
   "source": [
    "- **Intercept (-3.5)**: When `Attack`, `Legendary`, and `Generation_1` are all 0, the baseline log odds of the outcome (e.g., being socially connected) is -3.5.\n",
    "- **Attack Coefficient (0.1)**: For every 1-unit increase in `Attack`, the log odds of the outcome increase by 0.1. In simple terms, as `Attack` increases, the likelihood of being socially connected increases.\n",
    "- **Legendary Coefficient (1.2)**: If Legendary changes from 0 (not legendary) to 1 (legendary), the log odds of the outcome increase by 1.2. This means that `legendary` Pokémon are more likely to be socially connected than non-legendary ones.\n",
    "- **Generation_1 Coefficient (0.5)**: If `Generation_1` is 1 (belongs to Generation 1), the log odds of the outcome increase by 0.5 compared to Pokémon from other generations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4590f",
   "metadata": {},
   "source": [
    "### 4. Making Predictions (Approximation):\n",
    "Let’s say we want to predict the probability of a Pokémon being \"Fire\" type (`str8fyre = 1`) based on the values for `Attack`, `Legendary`, and `Generation_1`.\n",
    "\n",
    "For example, suppose we have a Pokémon with:\n",
    "\n",
    "- `Attack = 50`\n",
    "- `Legendary = 1` (legendary)\n",
    "- `Generation_1 = 1` (Generation 1)\n",
    "\n",
    "Using the simplified model:\n",
    "\n",
    "- $\\text{logit}(p)$ = −3.5+0.1×50+1.2×1+0.5×1\n",
    "- $\\text{logit}(p)$ = −3.5+5+1.2+0.5=3.2\n",
    "\n",
    "Now, convert the log odds to a probability using the logistic function:\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-3.2}} \\approx 0.96\n",
    "$$\n",
    "\n",
    "So, the predicted probability of this Pokémon being \"Fire\" type is approximately 96%.\n",
    "\n",
    "### 5. Conclusion:\n",
    "By interpreting the logistic regression model as a multivariate linear regression, we make predictions based on a linear combination of predictor variables, though technically, the coefficients represent changes in log odds, not direct probabilities. This approach simplifies the interpretation process, especially when you have a mixture of continuous and binary predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d0a04",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a5764f",
   "metadata": {},
   "source": [
    "When you fit a logistic regression model using smf.logit() in Python and call .fit(), you receive a summary table that contains statistical evidence for the predictor variables. Here’s how you can interpret the statistical evidence in the .summary() table, similar to what you would do for multiple linear regression.\n",
    "\n",
    "### Key Components of the .summary() Table for Logistic Regression:\n",
    "The summary table from .fit() contains several columns that provide statistical information. Here's how you can interpret each component:\n",
    "\n",
    "**1. Coef. (Coefficient)**:\n",
    "\n",
    "The coefficient values represent the effect of each predictor variable on the **log-odds** of the outcome. A positive coefficient means an increase in the predictor increases the log-odds of the event occurring (thus increasing the probability), while a negative coefficient means the opposite.\n",
    "\n",
    "**2. Std Err (Standard Error)**:\n",
    "The standard error represents the variability or uncertainty in the estimate of the coefficient. Smaller standard errors indicate more precise estimates.\n",
    "\n",
    "**3. z (z-Statistic)**:\n",
    "\n",
    "The z-statistic is calculated as the coefficient divided by its standard error. It tells you how many standard deviations the coefficient is away from zero. Higher absolute values of the z-statistic indicate stronger evidence against the null hypothesis (no effect).\n",
    "\n",
    "**4. P>|z| (P-value)**:\n",
    "\n",
    "The p-value tests the null hypothesis that the coefficient is equal to zero (no effect). A smaller p-value (typically below 0.05) indicates that the predictor is statistically significant and has a meaningful impact on the model. For example:\n",
    "- **p < 0.05**: The predictor is statistically significant.\n",
    "- **p ≥ 0.05**: The predictor is not statistically significant.\n",
    "\n",
    "**5. [0.025, 0.975] (Confidence Interval)**:\n",
    "\n",
    "This shows the 95% confidence interval for the coefficient. If the interval does not contain zero, it suggests that the coefficient is statistically significant at the 95% confidence level.\n",
    "\n",
    "### Example of How to Interpret the Results:\n",
    "Let’s assume you have the following output from the .summary() method:\n",
    "\n",
    "                          Logit Regression Results                           \n",
    "==============================================================================\n",
    "Dep. Variable:               target   No. Observations:                  100\n",
    "Model:                          Logit   Df Residuals:                      96\n",
    "Method:                           MLE   Df Model:                           3\n",
    "Date:                Tue, 12 Nov 2024   Pseudo R-squ.:                 0.225\n",
    "Time:                        15:30:00   Log-Likelihood:                -50.230\n",
    "converged:                       True   LL-Null:                       -64.125\n",
    "Covariance Type:            nonrobust   LLR p-value:                 1.25e-07\n",
    "==============================================================================\n",
    "                 Coef.   Std Err.      z    P>|z|   [0.025  0.975]\n",
    "------------------------------------------------------------------------------\n",
    "Intercept     -3.5000    1.2500    -2.800   0.005     -5.000   -2.000\n",
    "Attack         0.1000    0.0500     2.000   0.045      0.002    0.198\n",
    "Legendary      1.2000    0.5000     2.400   0.016      0.220    2.180\n",
    "Generation_1   0.5000    0.3000     1.667   0.095     -0.050    1.050\n",
    "==============================================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46bea2",
   "metadata": {},
   "source": [
    "### Interpretation:\n",
    "**1. Intercept (-3.5)**:\n",
    "\n",
    "The intercept represents the log-odds of the outcome when all predictor variables are zero. Since the p-value is **0.005**, which is less than 0.05, the intercept is statistically significant.\n",
    "\n",
    "**2. Attack (0.1)**:\n",
    "\n",
    "- For each one-unit increase in the **Attack** value, the log-odds of the outcome increase by 0.1. The p-value is **0.045**, which is less than 0.05, indicating that **Attack** is statistically significant.\n",
    "- The 95% confidence interval for the coefficient is **[0.002, 0.198]**, which does not contain zero, supporting the conclusion that **Attack** is a meaningful predictor.\n",
    "\n",
    "**3. Legendary (1.2)**:\n",
    "\n",
    "- The coefficient for **Legendary** means that if the Pokémon is legendary, the log-odds of the outcome increase by 1.2, compared to non-legendary Pokémon. The p-value is **0.016**, which is less than 0.05, indicating that **Legendary** is statistically significant.\n",
    "- The 95% confidence interval for the coefficient is **[0.220, 2.180]**, which does not contain zero, further confirming its significance.\n",
    "\n",
    "**4. Generation_1 (0.5)**:\n",
    "\n",
    "- The coefficient for **Generation_1** means that if the Pokémon is from Generation 1, the log-odds of the outcome increase by 0.5. The p-value is **0.095**, which is greater than 0.05, suggesting that **Generation_1** is not statistically significant at the 95% confidence level.\n",
    "- The 95% confidence interval for the coefficient is **[-0.050, 1.050]**, which contains zero, further suggesting that this predictor is not statistically significant.\n",
    "\n",
    "### Conclusion:\n",
    "- **Significant predictors**: **Attack** and **Legendary** have significant effects on the outcome, as their p-values are less than 0.05 and their confidence intervals do not include zero.\n",
    "- **Non-significant predictor**: **Generation_1** is not statistically significant, as its p-value is greater than 0.05 and its confidence interval includes zero.\n",
    "\n",
    "By using the `.summary()` table, you can make informed decisions about which predictors are meaningful and which might be excluded or need further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485334c",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd2ebd",
   "metadata": {},
   "source": [
    "To visualize the logistic regression model using Plotly while pretending it's a multivariate linear regression, we can simulate data and draw \"best fit lines\" under both additive and synergistic (interaction) specifications. This approach will allow you to understand the importance of the interaction term even in the context of logistic regression, although we are technically modeling log-odds.\n",
    "\n",
    "Here’s a step-by-step breakdown of how you can approach this:\n",
    "\n",
    "### Step 1: Import Required Libraries\n",
    "You'll need `plotly`, `pandas`, and `numpy` to create the visualizations. We'll also use `statsmodels` to generate the coefficients for the \"linear regression-like\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63ddd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe808fc4",
   "metadata": {},
   "source": [
    "### Step 2: Simulate the Data\n",
    "You can simulate some continuous and binary predictor data (e.g., `Attack` as continuous, `Legendary` as binary) and the outcome variable. For the sake of simplicity, we simulate a dataset with `Attack`, `Legendary`, and `Generation_1` as predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a2df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate continuous and binary predictor data\n",
    "np.random.seed(42)\n",
    "n = 200  # Number of data points\n",
    "\n",
    "Attack = np.random.normal(50, 10, n)  # Continuous variable\n",
    "Legendary = np.random.choice([0, 1], size=n)  # Binary variable\n",
    "Generation_1 = np.random.choice([0, 1], size=n)  # Binary variable for a second binary feature\n",
    "\n",
    "# Simulate log-odds as a function of the predictors (for logistic regression)\n",
    "log_odds = -3.5 + 0.1 * Attack + 1.2 * Legendary + 0.5 * Generation_1\n",
    "\n",
    "# Convert log-odds to probabilities using the logistic function\n",
    "probability = 1 / (1 + np.exp(-log_odds))\n",
    "\n",
    "# Simulate random noise to generate binary outcomes (0 or 1)\n",
    "outcome = np.random.binomial(1, probability)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "data = pd.DataFrame({\n",
    "    'Attack': Attack,\n",
    "    'Legendary': Legendary,\n",
    "    'Generation_1': Generation_1,\n",
    "    'Outcome': outcome\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccba74",
   "metadata": {},
   "source": [
    "### Step 3: Fit the Logistic Regression Model (Linear Assumption)\n",
    "Next, we’ll fit a logistic regression model and use the resulting coefficients to visualize the \"best fit lines\" under both additive and synergistic specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fca454cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.397157\n",
      "         Iterations 7\n",
      "Intercept      -1.851140\n",
      "Attack          0.060674\n",
      "Legendary       0.992232\n",
      "Generation_1    0.426119\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression model\n",
    "log_reg = smf.logit('Outcome ~ Attack + Legendary + Generation_1', data=data).fit()\n",
    "\n",
    "# Get the coefficients from the model\n",
    "coefficients = log_reg.params\n",
    "print(coefficients)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d600c52",
   "metadata": {},
   "source": [
    "### Step 4: Define the Additive and Synergistic Specifications\n",
    "- **Additive specification**: No interaction term; just the main effects.\n",
    "- **Synergistic specification**: Includes an interaction term between `Attack` and `Legendary` to examine potential synergistic effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf20e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additive model: No interaction term\n",
    "def additive_line(x):\n",
    "    return coefficients['Intercept'] + coefficients['Attack'] * x\n",
    "\n",
    "# Synergistic model: With interaction between Attack and Legendary\n",
    "def synergistic_line(x):\n",
    "    return coefficients['Intercept'] + coefficients['Attack'] * x + coefficients['Legendary'] * 1  # assuming Legendary=1 for simplicity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5566b87a",
   "metadata": {},
   "source": [
    "### Step 5: Plot the Additive and Synergistic Best Fit Lines\n",
    "We’ll use `plotly.graph_objects` to plot the data and best fit lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7877ee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate a range of values for the continuous predictor (Attack)\n",
    "attack_range = np.linspace(min(data['Attack']), max(data['Attack']), 100)\n",
    "\n",
    "# Add noise to the data for visualization purposes\n",
    "noise = np.random.normal(0, 0.2, size=n)\n",
    "\n",
    "# Plot Additive Specification\n",
    "fig_additive = go.Figure()\n",
    "\n",
    "# Add the data points\n",
    "fig_additive.add_trace(go.Scatter(x=data['Attack'], y=data['Outcome'] + noise, mode='markers', name='Data'))\n",
    "\n",
    "# Add the additive best fit line\n",
    "fig_additive.add_trace(go.Scatter(x=attack_range, y=additive_line(attack_range), mode='lines', name='Additive Best Fit'))\n",
    "\n",
    "fig_additive.update_layout(title=\"Additive Specification (No Interaction)\",\n",
    "                           xaxis_title=\"Attack\",\n",
    "                           yaxis_title=\"Outcome\")\n",
    "\n",
    "# Plot Synergistic Specification\n",
    "fig_synergistic = go.Figure()\n",
    "\n",
    "# Add the data points\n",
    "fig_synergistic.add_trace(go.Scatter(x=data['Attack'], y=data['Outcome'] + noise, mode='markers', name='Data'))\n",
    "\n",
    "# Add the synergistic best fit line\n",
    "fig_synergistic.add_trace(go.Scatter(x=attack_range, y=synergistic_line(attack_range), mode='lines', name='Synergistic Best Fit'))\n",
    "\n",
    "fig_synergistic.update_layout(title=\"Synergistic Specification (With Interaction)\",\n",
    "                              xaxis_title=\"Attack\",\n",
    "                              yaxis_title=\"Outcome\")\n",
    "\n",
    "# Show the plots\n",
    "fig_additive.show(renderer=\"png\")\n",
    "fig_synergistic.show(renderer=\"png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a584a184",
   "metadata": {},
   "source": [
    "### Step 6: Comment on the Necessity of the Interaction Term\n",
    "- **Additive Model**: The \"best fit line\" in the additive model will show the relationship between `Attack` and the outcome, treating `Legendary` and `Generation_1` as constants.\n",
    "- **Synergistic Model**: The synergistic model includes an interaction between `Attack` and `Legendary`, which might change the relationship between `Attack` and the outcome depending on whether `Legendary` is 0 or 1.\n",
    "\n",
    "After visualizing the lines, compare the shapes of the two lines. If the lines in the additive model are relatively flat or similar for both categories of `Legendary`, then the interaction term may not be necessary. If the lines in the synergistic model show significantly different slopes depending on the value of `Legendary`, then the interaction term is likely important.\n",
    "\n",
    "### Conclusion:\n",
    "By pretending this is a linear regression model, we can visualize the potential effect of adding interaction terms. Even though logistic regression involves log-odds, this approach provides a way to interpret the relationships visually. After comparing the plots, you’ll be able to assess the importance of the interaction term based on how the best fit lines differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ac42b",
   "metadata": {},
   "source": [
    "**[Link to the ChatBot Session](https://chatgpt.com/share/67325beb-d074-800e-bb0a-56a62175467c)**\n",
    "\n",
    "**Summary of the ChatBot Session**\n",
    "Here’s a summary of our exchanges during this session:\n",
    "\n",
    "Logistic Regression Guidance: You asked for guidance on performing logistic regression using a dataset (from the Canadian Social Connection Survey) to model categorical or binary outcomes instead of continuous ones. I helped you set up a logistic regression model in Python, specifically using statsmodels.formula.api. You were also provided with a formula for logistic regression and an explanation of how to interpret the coefficients.\n",
    "\n",
    "Formula Representation in Markdown: You inquired about how to type logistic regression formulas and predictions in Markdown within JupyterHub. I explained how to properly format the logistic regression formula using LaTeX syntax (\\text{logit}(p) and similar) for both the log-odds and probability conversions. I also provided guidance on how to display the logistic function equation and predictions in Markdown cells in JupyterHub.\n",
    "\n",
    "Interpretation of Coefficients: We discussed interpreting the logistic regression results and the difficulty of interpreting log-odds compared to linear regression. I recommended interpreting the logistic regression models as if they were multivariate linear regression models for simplicity, and we walked through interpreting the statistical evidence associated with predictor variables in a logistic regression output summary.\n",
    "\n",
    "Data Visualization with Plotly: You asked how to visualize the logistic regression model's results using Plotly, specifically how to plot \"best fit lines\" for both additive and synergistic models. I walked you through generating simulated data, fitting a logistic regression model, and plotting the best fit lines in Plotly under both an additive model (without interaction) and a synergistic model (with interaction). I also explained how to interpret whether the interaction term is necessary by comparing the two visualizations.\n",
    "\n",
    "This summary covers all the main points and steps we discussed related to logistic regression, interpreting model results, and visualizing them using Plotly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb64f25",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e262c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>719</td>\n",
       "      <td>Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>719</td>\n",
       "      <td>DiancieMega Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Unbound</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Dark</td>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>721</td>\n",
       "      <td>Volcanion</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Water</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                   Name   Type 1  Type 2  HP  Attack  Defense  \\\n",
       "0      1              Bulbasaur    Grass  Poison  45      49       49   \n",
       "1      2                Ivysaur    Grass  Poison  60      62       63   \n",
       "2      3               Venusaur    Grass  Poison  80      82       83   \n",
       "3      3  VenusaurMega Venusaur    Grass  Poison  80     100      123   \n",
       "4      4             Charmander     Fire     NaN  39      52       43   \n",
       "..   ...                    ...      ...     ...  ..     ...      ...   \n",
       "795  719                Diancie     Rock   Fairy  50     100      150   \n",
       "796  719    DiancieMega Diancie     Rock   Fairy  50     160      110   \n",
       "797  720    HoopaHoopa Confined  Psychic   Ghost  80     110       60   \n",
       "798  720     HoopaHoopa Unbound  Psychic    Dark  80     160       60   \n",
       "799  721              Volcanion     Fire   Water  80     110      120   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0         65       65     45           1      False  \n",
       "1         80       80     60           1      False  \n",
       "2        100      100     80           1      False  \n",
       "3        122      120     80           1      False  \n",
       "4         60       50     65           1      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "795      100      150     50           6       True  \n",
       "796      160      110    110           6       True  \n",
       "797      150      130     70           6       True  \n",
       "798      170      130     80           6       True  \n",
       "799      130       90     70           6       True  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) \n",
    "pokeaman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57fdcc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 11 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:23:28</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Mon, 11 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     18:23:28     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Mon, 11 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        18:23:28   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152778e4",
   "metadata": {},
   "source": [
    "In interpreting the model output, it’s important to recognize that R-squared values and p-values provide insights into distinct aspects of the model, and they are not inherently contradictory. They serve different purposes in evaluating model fit and the significance of predictor variables.\n",
    "\n",
    "**1. R-squared and Overall Model Fit**\n",
    "- **R-squared** measures the proportion of the variability in the dependent variable (here, \"HP\") that is explained by the model. An R-squared value of 17.6% indicates that the model accounts for only a modest portion of the total variability in \"HP\" values. This suggests that while some of the variability in \"HP\" can be attributed to \"Sp. Def\" and \"Generation,\" a substantial portion remains unexplained by these predictors alone. Thus, there may be other relevant predictors (not included in this model) that influence \"HP,\" or the relationship between \"HP\" and the included predictors may not be strictly linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82c49d5",
   "metadata": {},
   "source": [
    "**2. Significance of Coefficients (P-values)**\n",
    "- **P-values** assess the statistical significance of each predictor variable's coefficient. A small p-value indicates strong evidence against the null hypothesis that a predictor has \"no effect\" on the outcome, in this case \"HP.\" Despite the low R-squared, the model could still contain statistically significant predictors (with low p-values) that have meaningful, non-zero effects on the outcome variable. In this model, some coefficients are larger than 10 and have low p-values, suggesting that \"Sp. Def\" and \"Generation\" have statistically significant effects on \"HP,\" even though they do not explain a large portion of its variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4334ed03",
   "metadata": {},
   "source": [
    "**3. Complementary Insights**\n",
    "- **R-squared and p-values address different questions**:\n",
    "    - R-squared tells us how well the model explains the variability in the outcome overall, providing insight into the model’s explanatory power as a whole.\n",
    "    - P-values focus on individual predictors and tell us if there is enough evidence to conclude that each predictor has a statistically significant effect on the outcome.\n",
    "- Therefore, a model can have significant coefficients (low p-values) while still explaining only a modest portion of the total variability in the outcome (low R-squared). This would mean that although the predictors have significant effects, they don’t explain all factors that influence \"HP.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f3916",
   "metadata": {},
   "source": [
    "**4. Use of Categorical Variables and Interactions**\n",
    "- The formula `HP ~ Q(\"Sp. Def\") * C(Generation)` models \"Generation\" as a categorical variable, creating dummy variables for each generation. Interpreting \"Generation\" as categorical rather than continuous is appropriate here since the numeric values of \"Generation\" (1 through 6) simply label distinct groups rather than representing a scale.\n",
    "- The interaction term `(Q(\"Sp. Def\") * C(Generation))` allows the model to capture how the relationship between \"Sp. Def\" and \"HP\" may vary across different generations. This is more flexible than a simple linear predictor and could reveal differences that add insight into how \"Generation\" interacts with \"Sp. Def.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ed394",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "In summary, the low R-squared value highlights limited explanatory power of the model as a whole, while the significant coefficients with low p-values indicate that the included predictors are indeed meaningful and have statistically significant effects on the outcome. Together, these insights suggest that the predictors in the model are influential but may only capture part of the complex factors that determine \"HP\" in Pokémon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d77a2d",
   "metadata": {},
   "source": [
    "**[Link to the ChatBot Session](https://chatgpt.com/share/67324adf-ac90-800e-909f-46059dd1e823)**\n",
    "\n",
    "**Summary of the ChatBot Session**\n",
    "\n",
    "We discussed the interpretation of low R-squared values and significant p-values in a linear regression model. You provided a scenario involving a model with an R-squared of 17.6%, indicating limited explanatory power, but with some coefficients greater than 10 and statistically significant p-values. I explained that R-squared measures the proportion of the outcome’s variability explained by the model, while p-values assess the evidence against each predictor’s null hypothesis of “no effect.”\n",
    "\n",
    "We explored how these metrics complement each other rather than conflict, with R-squared offering insight into the model’s overall explanatory strength and p-values focusing on the significance of individual predictors. Despite a low R-squared, the presence of significant p-values suggests that \"Sp. Def\" and \"Generation\" have meaningful, though limited, explanatory effects on the outcome (\"HP\"). Additionally, we discussed the model specification HP ~ Q(\"Sp. Def\") * C(Generation), explaining the use of Q() for accessing column names with spaces and C() for treating \"Generation\" as a categorical predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae60bdd8",
   "metadata": {},
   "source": [
    "# Post-Lecture HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6a88e",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bee7a0",
   "metadata": {},
   "source": [
    "### Given codes below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2984e897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>338</td>\n",
       "      <td>Solrock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Charizard</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>109</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>224</td>\n",
       "      <td>Octillery</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>600</td>\n",
       "      <td>Klang</td>\n",
       "      <td>Steel</td>\n",
       "      <td>None</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>265</td>\n",
       "      <td>Wurmple</td>\n",
       "      <td>Bug</td>\n",
       "      <td>None</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>471</td>\n",
       "      <td>Glaceon</td>\n",
       "      <td>Ice</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>130</td>\n",
       "      <td>95</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>225</td>\n",
       "      <td>Delibird</td>\n",
       "      <td>Ice</td>\n",
       "      <td>Flying</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>109</td>\n",
       "      <td>Koffing</td>\n",
       "      <td>Poison</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>373</td>\n",
       "      <td>SalamenceMega Salamence</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Flying</td>\n",
       "      <td>95</td>\n",
       "      <td>145</td>\n",
       "      <td>130</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                     Name   Type 1   Type 2  HP  Attack  Defense  \\\n",
       "370  338                  Solrock     Rock  Psychic  70      95       85   \n",
       "6      6                Charizard     Fire   Flying  78      84       78   \n",
       "242  224                Octillery    Water     None  75     105       75   \n",
       "661  600                    Klang    Steel     None  60      80       95   \n",
       "288  265                  Wurmple      Bug     None  45      45       35   \n",
       "..   ...                      ...      ...      ...  ..     ...      ...   \n",
       "522  471                  Glaceon      Ice     None  65      60      110   \n",
       "243  225                 Delibird      Ice   Flying  45      55       45   \n",
       "797  720      HoopaHoopa Confined  Psychic    Ghost  80     110       60   \n",
       "117  109                  Koffing   Poison     None  40      65       95   \n",
       "409  373  SalamenceMega Salamence   Dragon   Flying  95     145      130   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "370       55       65     70           3      False  \n",
       "6        109       85    100           1      False  \n",
       "242      105       75     45           2      False  \n",
       "661       70       85     50           5      False  \n",
       "288       20       30     20           3      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "522      130       95     65           4      False  \n",
       "243       65       45     75           2      False  \n",
       "797      150      130     70           6       True  \n",
       "117       60       45     35           1      False  \n",
       "409      120       90    120           3      False  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d461bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 11 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:24:31</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Mon, 11 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     18:24:31     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Mon, 11 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        18:24:31   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d95c7b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n"
     ]
    }
   ],
   "source": [
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ddb1070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 11 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.23e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>18:24:53</td>     <th>  Log-Likelihood:    </th> <td> -1738.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3603.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   337</td>      <th>  BIC:               </th> <td>   3855.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    62</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                  <td></td>                                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                        <td>  521.5715</td> <td>  130.273</td> <td>    4.004</td> <td> 0.000</td> <td>  265.322</td> <td>  777.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                                                <td>   -6.1179</td> <td>    2.846</td> <td>   -2.150</td> <td> 0.032</td> <td>  -11.716</td> <td>   -0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                                           <td>   -8.1938</td> <td>    2.329</td> <td>   -3.518</td> <td> 0.000</td> <td>  -12.775</td> <td>   -3.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                                         <td>-1224.9610</td> <td>  545.105</td> <td>   -2.247</td> <td> 0.025</td> <td>-2297.199</td> <td> -152.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                                          <td>   -6.1989</td> <td>    2.174</td> <td>   -2.851</td> <td> 0.005</td> <td>  -10.475</td> <td>   -1.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]</th>                                        <td> -102.4030</td> <td>   96.565</td> <td>   -1.060</td> <td> 0.290</td> <td> -292.350</td> <td>   87.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense</th>                                                   <td>    0.0985</td> <td>    0.033</td> <td>    2.982</td> <td> 0.003</td> <td>    0.034</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]</th>                                 <td>   14.6361</td> <td>    6.267</td> <td>    2.336</td> <td> 0.020</td> <td>    2.310</td> <td>   26.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                                            <td>   -7.2261</td> <td>    2.178</td> <td>   -3.318</td> <td> 0.001</td> <td>  -11.511</td> <td>   -2.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]</th>                                          <td>  704.8798</td> <td>  337.855</td> <td>    2.086</td> <td> 0.038</td> <td>   40.309</td> <td> 1369.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                                                     <td>    0.1264</td> <td>    0.038</td> <td>    3.351</td> <td> 0.001</td> <td>    0.052</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]</th>                                   <td>    5.8648</td> <td>    2.692</td> <td>    2.179</td> <td> 0.030</td> <td>    0.570</td> <td>   11.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed</th>                                                    <td>    0.1026</td> <td>    0.039</td> <td>    2.634</td> <td> 0.009</td> <td>    0.026</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]</th>                                  <td>   -6.9266</td> <td>    3.465</td> <td>   -1.999</td> <td> 0.046</td> <td>  -13.742</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed</th>                                             <td>   -0.0016</td> <td>    0.001</td> <td>   -2.837</td> <td> 0.005</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]</th>                           <td>   -0.0743</td> <td>    0.030</td> <td>   -2.477</td> <td> 0.014</td> <td>   -0.133</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                                                     <td>   -5.3982</td> <td>    1.938</td> <td>   -2.785</td> <td> 0.006</td> <td>   -9.211</td> <td>   -1.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\")</th>                                   <td> -282.2496</td> <td>  126.835</td> <td>   -2.225</td> <td> 0.027</td> <td> -531.738</td> <td>  -32.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                                              <td>    0.1094</td> <td>    0.034</td> <td>    3.233</td> <td> 0.001</td> <td>    0.043</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\")</th>                            <td>   12.6503</td> <td>    5.851</td> <td>    2.162</td> <td> 0.031</td> <td>    1.141</td> <td>   24.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\")</th>                                             <td>    0.0628</td> <td>    0.028</td> <td>    2.247</td> <td> 0.025</td> <td>    0.008</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                           <td>    3.3949</td> <td>    1.783</td> <td>    1.904</td> <td> 0.058</td> <td>   -0.112</td> <td>    6.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\")</th>                                      <td>   -0.0012</td> <td>    0.000</td> <td>   -2.730</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                    <td>   -0.1456</td> <td>    0.065</td> <td>   -2.253</td> <td> 0.025</td> <td>   -0.273</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                                               <td>    0.0624</td> <td>    0.031</td> <td>    2.027</td> <td> 0.043</td> <td>    0.002</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                             <td>   -3.2219</td> <td>    1.983</td> <td>   -1.625</td> <td> 0.105</td> <td>   -7.122</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>                                        <td>   -0.0014</td> <td>    0.001</td> <td>   -2.732</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                      <td>   -0.0695</td> <td>    0.033</td> <td>   -2.100</td> <td> 0.036</td> <td>   -0.135</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\")</th>                                       <td>   -0.0008</td> <td>    0.000</td> <td>   -1.743</td> <td> 0.082</td> <td>   -0.002</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                     <td>    0.0334</td> <td>    0.021</td> <td>    1.569</td> <td> 0.117</td> <td>   -0.008</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\")</th>                                <td> 1.629e-05</td> <td> 6.92e-06</td> <td>    2.355</td> <td> 0.019</td> <td> 2.68e-06</td> <td> 2.99e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>              <td>    0.0008</td> <td>    0.000</td> <td>    2.433</td> <td> 0.015</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                                                     <td>   -8.3636</td> <td>    2.346</td> <td>   -3.565</td> <td> 0.000</td> <td>  -12.978</td> <td>   -3.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Atk\")</th>                                   <td>  850.5436</td> <td>  385.064</td> <td>    2.209</td> <td> 0.028</td> <td>   93.112</td> <td> 1607.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                                              <td>    0.1388</td> <td>    0.040</td> <td>    3.500</td> <td> 0.001</td> <td>    0.061</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Atk\")</th>                            <td>    2.1809</td> <td>    1.136</td> <td>    1.920</td> <td> 0.056</td> <td>   -0.054</td> <td>    4.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Atk\")</th>                                             <td>    0.0831</td> <td>    0.038</td> <td>    2.162</td> <td> 0.031</td> <td>    0.007</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                           <td>   -7.3121</td> <td>    3.376</td> <td>   -2.166</td> <td> 0.031</td> <td>  -13.953</td> <td>   -0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Atk\")</th>                                      <td>   -0.0014</td> <td>    0.001</td> <td>   -2.480</td> <td> 0.014</td> <td>   -0.003</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                    <td>   -0.0434</td> <td>    0.022</td> <td>   -2.010</td> <td> 0.045</td> <td>   -0.086</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                                               <td>    0.1011</td> <td>    0.035</td> <td>    2.872</td> <td> 0.004</td> <td>    0.032</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                             <td>  -12.6343</td> <td>    5.613</td> <td>   -2.251</td> <td> 0.025</td> <td>  -23.674</td> <td>   -1.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>                                        <td>   -0.0018</td> <td>    0.001</td> <td>   -3.102</td> <td> 0.002</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                      <td>    0.0151</td> <td>    0.009</td> <td>    1.609</td> <td> 0.109</td> <td>   -0.003</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Atk\")</th>                                       <td>   -0.0012</td> <td>    0.001</td> <td>   -1.860</td> <td> 0.064</td> <td>   -0.002</td> <td> 6.62e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                     <td>    0.1210</td> <td>    0.054</td> <td>    2.260</td> <td> 0.024</td> <td>    0.016</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Atk\")</th>                                <td> 2.125e-05</td> <td>  9.1e-06</td> <td>    2.334</td> <td> 0.020</td> <td> 3.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>              <td> 6.438e-06</td> <td> 7.69e-05</td> <td>    0.084</td> <td> 0.933</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                        <td>    0.1265</td> <td>    0.033</td> <td>    3.821</td> <td> 0.000</td> <td>    0.061</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                      <td>   -5.0544</td> <td>    2.506</td> <td>   -2.017</td> <td> 0.044</td> <td>   -9.983</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                 <td>   -0.0021</td> <td>    0.001</td> <td>   -3.606</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>               <td>   -0.0346</td> <td>    0.017</td> <td>   -1.992</td> <td> 0.047</td> <td>   -0.069</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                <td>   -0.0012</td> <td>    0.000</td> <td>   -2.406</td> <td> 0.017</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0446</td> <td>    0.025</td> <td>    1.794</td> <td> 0.074</td> <td>   -0.004</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                         <td> 1.973e-05</td> <td> 7.28e-06</td> <td>    2.710</td> <td> 0.007</td> <td> 5.41e-06</td> <td>  3.4e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>    0.0005</td> <td>    0.000</td> <td>    1.957</td> <td> 0.051</td> <td>-2.56e-06</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                  <td>   -0.0013</td> <td>    0.000</td> <td>   -2.740</td> <td> 0.006</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                <td>    0.0841</td> <td>    0.040</td> <td>    2.125</td> <td> 0.034</td> <td>    0.006</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                           <td> 2.379e-05</td> <td> 7.85e-06</td> <td>    3.030</td> <td> 0.003</td> <td> 8.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>         <td> 2.864e-05</td> <td> 7.73e-05</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                          <td> 1.284e-05</td> <td> 7.46e-06</td> <td>    1.721</td> <td> 0.086</td> <td>-1.83e-06</td> <td> 2.75e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0008</td> <td>    0.000</td> <td>   -2.085</td> <td> 0.038</td> <td>   -0.002</td> <td>-4.68e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                   <td> -2.53e-07</td> <td>  1.1e-07</td> <td>   -2.292</td> <td> 0.023</td> <td> -4.7e-07</td> <td>-3.59e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>-1.425e-06</td> <td> 1.14e-06</td> <td>   -1.249</td> <td> 0.212</td> <td>-3.67e-06</td> <td> 8.19e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.2e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                                   &        HP        & \\textbf{  R-squared:         } &     0.467   \\\\\n",
       "\\textbf{Model:}                                                           &       OLS        & \\textbf{  Adj. R-squared:    } &     0.369   \\\\\n",
       "\\textbf{Method:}                                                          &  Least Squares   & \\textbf{  F-statistic:       } &     4.764   \\\\\n",
       "\\textbf{Date:}                                                            & Mon, 11 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.23e-21   \\\\\n",
       "\\textbf{Time:}                                                            &     18:24:53     & \\textbf{  Log-Likelihood:    } &   -1738.6   \\\\\n",
       "\\textbf{No. Observations:}                                                &         400      & \\textbf{  AIC:               } &     3603.   \\\\\n",
       "\\textbf{Df Residuals:}                                                    &         337      & \\textbf{  BIC:               } &     3855.   \\\\\n",
       "\\textbf{Df Model:}                                                        &          62      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                                                 &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                                        &     521.5715  &      130.273     &     4.004  &         0.000        &      265.322    &      777.821     \\\\\n",
       "\\textbf{Legendary[T.True]}                                                &      -6.1179  &        2.846     &    -2.150  &         0.032        &      -11.716    &       -0.520     \\\\\n",
       "\\textbf{Attack}                                                           &      -8.1938  &        2.329     &    -3.518  &         0.000        &      -12.775    &       -3.612     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                                         &   -1224.9610  &      545.105     &    -2.247  &         0.025        &    -2297.199    &     -152.723     \\\\\n",
       "\\textbf{Defense}                                                          &      -6.1989  &        2.174     &    -2.851  &         0.005        &      -10.475    &       -1.923     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]}                                        &    -102.4030  &       96.565     &    -1.060  &         0.290        &     -292.350    &       87.544     \\\\\n",
       "\\textbf{Attack:Defense}                                                   &       0.0985  &        0.033     &     2.982  &         0.003        &        0.034    &        0.164     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]}                                 &      14.6361  &        6.267     &     2.336  &         0.020        &        2.310    &       26.963     \\\\\n",
       "\\textbf{Speed}                                                            &      -7.2261  &        2.178     &    -3.318  &         0.001        &      -11.511    &       -2.942     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]}                                          &     704.8798  &      337.855     &     2.086  &         0.038        &       40.309    &     1369.450     \\\\\n",
       "\\textbf{Attack:Speed}                                                     &       0.1264  &        0.038     &     3.351  &         0.001        &        0.052    &        0.201     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]}                                   &       5.8648  &        2.692     &     2.179  &         0.030        &        0.570    &       11.160     \\\\\n",
       "\\textbf{Defense:Speed}                                                    &       0.1026  &        0.039     &     2.634  &         0.009        &        0.026    &        0.179     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]}                                  &      -6.9266  &        3.465     &    -1.999  &         0.046        &      -13.742    &       -0.111     \\\\\n",
       "\\textbf{Attack:Defense:Speed}                                             &      -0.0016  &        0.001     &    -2.837  &         0.005        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]}                           &      -0.0743  &        0.030     &    -2.477  &         0.014        &       -0.133    &       -0.015     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                                                     &      -5.3982  &        1.938     &    -2.785  &         0.006        &       -9.211    &       -1.586     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\")}                                   &    -282.2496  &      126.835     &    -2.225  &         0.027        &     -531.738    &      -32.761     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                                              &       0.1094  &        0.034     &     3.233  &         0.001        &        0.043    &        0.176     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\")}                            &      12.6503  &        5.851     &     2.162  &         0.031        &        1.141    &       24.160     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\")}                                             &       0.0628  &        0.028     &     2.247  &         0.025        &        0.008    &        0.118     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\")}                           &       3.3949  &        1.783     &     1.904  &         0.058        &       -0.112    &        6.902     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\")}                                      &      -0.0012  &        0.000     &    -2.730  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")}                    &      -0.1456  &        0.065     &    -2.253  &         0.025        &       -0.273    &       -0.018     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                                               &       0.0624  &        0.031     &     2.027  &         0.043        &        0.002    &        0.123     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\")}                             &      -3.2219  &        1.983     &    -1.625  &         0.105        &       -7.122    &        0.678     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}                                        &      -0.0014  &        0.001     &    -2.732  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                      &      -0.0695  &        0.033     &    -2.100  &         0.036        &       -0.135    &       -0.004     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\")}                                       &      -0.0008  &        0.000     &    -1.743  &         0.082        &       -0.002    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                     &       0.0334  &        0.021     &     1.569  &         0.117        &       -0.008    &        0.075     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\")}                                &    1.629e-05  &     6.92e-06     &     2.355  &         0.019        &     2.68e-06    &     2.99e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}              &       0.0008  &        0.000     &     2.433  &         0.015        &        0.000    &        0.001     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                                                     &      -8.3636  &        2.346     &    -3.565  &         0.000        &      -12.978    &       -3.749     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Atk\")}                                   &     850.5436  &      385.064     &     2.209  &         0.028        &       93.112    &     1607.975     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                                              &       0.1388  &        0.040     &     3.500  &         0.001        &        0.061    &        0.217     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Atk\")}                            &       2.1809  &        1.136     &     1.920  &         0.056        &       -0.054    &        4.416     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Atk\")}                                             &       0.0831  &        0.038     &     2.162  &         0.031        &        0.007    &        0.159     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                           &      -7.3121  &        3.376     &    -2.166  &         0.031        &      -13.953    &       -0.671     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Atk\")}                                      &      -0.0014  &        0.001     &    -2.480  &         0.014        &       -0.003    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                    &      -0.0434  &        0.022     &    -2.010  &         0.045        &       -0.086    &       -0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                                               &       0.1011  &        0.035     &     2.872  &         0.004        &        0.032    &        0.170     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                             &     -12.6343  &        5.613     &    -2.251  &         0.025        &      -23.674    &       -1.594     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}                                        &      -0.0018  &        0.001     &    -3.102  &         0.002        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                      &       0.0151  &        0.009     &     1.609  &         0.109        &       -0.003    &        0.034     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Atk\")}                                       &      -0.0012  &        0.001     &    -1.860  &         0.064        &       -0.002    &     6.62e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                     &       0.1210  &        0.054     &     2.260  &         0.024        &        0.016    &        0.226     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Atk\")}                                &    2.125e-05  &      9.1e-06     &     2.334  &         0.020        &     3.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}              &    6.438e-06  &     7.69e-05     &     0.084  &         0.933        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                        &       0.1265  &        0.033     &     3.821  &         0.000        &        0.061    &        0.192     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                      &      -5.0544  &        2.506     &    -2.017  &         0.044        &       -9.983    &       -0.126     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                 &      -0.0021  &        0.001     &    -3.606  &         0.000        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}               &      -0.0346  &        0.017     &    -1.992  &         0.047        &       -0.069    &       -0.000     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                &      -0.0012  &        0.000     &    -2.406  &         0.017        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0446  &        0.025     &     1.794  &         0.074        &       -0.004    &        0.093     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                         &    1.973e-05  &     7.28e-06     &     2.710  &         0.007        &     5.41e-06    &      3.4e-05     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &       0.0005  &        0.000     &     1.957  &         0.051        &    -2.56e-06    &        0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                  &      -0.0013  &        0.000     &    -2.740  &         0.006        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                &       0.0841  &        0.040     &     2.125  &         0.034        &        0.006    &        0.162     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                           &    2.379e-05  &     7.85e-06     &     3.030  &         0.003        &     8.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}         &    2.864e-05  &     7.73e-05     &     0.370  &         0.711        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                          &    1.284e-05  &     7.46e-06     &     1.721  &         0.086        &    -1.83e-06    &     2.75e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0008  &        0.000     &    -2.085  &         0.038        &       -0.002    &    -4.68e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                   &    -2.53e-07  &      1.1e-07     &    -2.292  &         0.023        &     -4.7e-07    &    -3.59e-08     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &   -1.425e-06  &     1.14e-06     &    -1.249  &         0.212        &    -3.67e-06    &     8.19e-07     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.2e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.467\n",
       "Model:                            OLS   Adj. R-squared:                  0.369\n",
       "Method:                 Least Squares   F-statistic:                     4.764\n",
       "Date:                Mon, 11 Nov 2024   Prob (F-statistic):           4.23e-21\n",
       "Time:                        18:24:53   Log-Likelihood:                -1738.6\n",
       "No. Observations:                 400   AIC:                             3603.\n",
       "Df Residuals:                     337   BIC:                             3855.\n",
       "Df Model:                          62                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================================================================\n",
       "                                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                          521.5715    130.273      4.004      0.000     265.322     777.821\n",
       "Legendary[T.True]                                                   -6.1179      2.846     -2.150      0.032     -11.716      -0.520\n",
       "Attack                                                              -8.1938      2.329     -3.518      0.000     -12.775      -3.612\n",
       "Attack:Legendary[T.True]                                         -1224.9610    545.105     -2.247      0.025   -2297.199    -152.723\n",
       "Defense                                                             -6.1989      2.174     -2.851      0.005     -10.475      -1.923\n",
       "Defense:Legendary[T.True]                                         -102.4030     96.565     -1.060      0.290    -292.350      87.544\n",
       "Attack:Defense                                                       0.0985      0.033      2.982      0.003       0.034       0.164\n",
       "Attack:Defense:Legendary[T.True]                                    14.6361      6.267      2.336      0.020       2.310      26.963\n",
       "Speed                                                               -7.2261      2.178     -3.318      0.001     -11.511      -2.942\n",
       "Speed:Legendary[T.True]                                            704.8798    337.855      2.086      0.038      40.309    1369.450\n",
       "Attack:Speed                                                         0.1264      0.038      3.351      0.001       0.052       0.201\n",
       "Attack:Speed:Legendary[T.True]                                       5.8648      2.692      2.179      0.030       0.570      11.160\n",
       "Defense:Speed                                                        0.1026      0.039      2.634      0.009       0.026       0.179\n",
       "Defense:Speed:Legendary[T.True]                                     -6.9266      3.465     -1.999      0.046     -13.742      -0.111\n",
       "Attack:Defense:Speed                                                -0.0016      0.001     -2.837      0.005      -0.003      -0.001\n",
       "Attack:Defense:Speed:Legendary[T.True]                              -0.0743      0.030     -2.477      0.014      -0.133      -0.015\n",
       "Q(\"Sp. Def\")                                                        -5.3982      1.938     -2.785      0.006      -9.211      -1.586\n",
       "Legendary[T.True]:Q(\"Sp. Def\")                                    -282.2496    126.835     -2.225      0.027    -531.738     -32.761\n",
       "Attack:Q(\"Sp. Def\")                                                  0.1094      0.034      3.233      0.001       0.043       0.176\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\")                               12.6503      5.851      2.162      0.031       1.141      24.160\n",
       "Defense:Q(\"Sp. Def\")                                                 0.0628      0.028      2.247      0.025       0.008       0.118\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\")                               3.3949      1.783      1.904      0.058      -0.112       6.902\n",
       "Attack:Defense:Q(\"Sp. Def\")                                         -0.0012      0.000     -2.730      0.007      -0.002      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")                       -0.1456      0.065     -2.253      0.025      -0.273      -0.018\n",
       "Speed:Q(\"Sp. Def\")                                                   0.0624      0.031      2.027      0.043       0.002       0.123\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\")                                -3.2219      1.983     -1.625      0.105      -7.122       0.678\n",
       "Attack:Speed:Q(\"Sp. Def\")                                           -0.0014      0.001     -2.732      0.007      -0.002      -0.000\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         -0.0695      0.033     -2.100      0.036      -0.135      -0.004\n",
       "Defense:Speed:Q(\"Sp. Def\")                                          -0.0008      0.000     -1.743      0.082      -0.002       0.000\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         0.0334      0.021      1.569      0.117      -0.008       0.075\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\")                                 1.629e-05   6.92e-06      2.355      0.019    2.68e-06    2.99e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                  0.0008      0.000      2.433      0.015       0.000       0.001\n",
       "Q(\"Sp. Atk\")                                                        -8.3636      2.346     -3.565      0.000     -12.978      -3.749\n",
       "Legendary[T.True]:Q(\"Sp. Atk\")                                     850.5436    385.064      2.209      0.028      93.112    1607.975\n",
       "Attack:Q(\"Sp. Atk\")                                                  0.1388      0.040      3.500      0.001       0.061       0.217\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Atk\")                                2.1809      1.136      1.920      0.056      -0.054       4.416\n",
       "Defense:Q(\"Sp. Atk\")                                                 0.0831      0.038      2.162      0.031       0.007       0.159\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Atk\")                              -7.3121      3.376     -2.166      0.031     -13.953      -0.671\n",
       "Attack:Defense:Q(\"Sp. Atk\")                                         -0.0014      0.001     -2.480      0.014      -0.003      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")                       -0.0434      0.022     -2.010      0.045      -0.086      -0.001\n",
       "Speed:Q(\"Sp. Atk\")                                                   0.1011      0.035      2.872      0.004       0.032       0.170\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Atk\")                               -12.6343      5.613     -2.251      0.025     -23.674      -1.594\n",
       "Attack:Speed:Q(\"Sp. Atk\")                                           -0.0018      0.001     -3.102      0.002      -0.003      -0.001\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                          0.0151      0.009      1.609      0.109      -0.003       0.034\n",
       "Defense:Speed:Q(\"Sp. Atk\")                                          -0.0012      0.001     -1.860      0.064      -0.002    6.62e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                         0.1210      0.054      2.260      0.024       0.016       0.226\n",
       "Attack:Defense:Speed:Q(\"Sp. Atk\")                                 2.125e-05    9.1e-06      2.334      0.020    3.34e-06    3.92e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")               6.438e-06   7.69e-05      0.084      0.933      -0.000       0.000\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                            0.1265      0.033      3.821      0.000       0.061       0.192\n",
       "Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                         -5.0544      2.506     -2.017      0.044      -9.983      -0.126\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                    -0.0021      0.001     -3.606      0.000      -0.003      -0.001\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  -0.0346      0.017     -1.992      0.047      -0.069      -0.000\n",
       "Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                   -0.0012      0.000     -2.406      0.017      -0.002      -0.000\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0446      0.025      1.794      0.074      -0.004       0.093\n",
       "Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                          1.973e-05   7.28e-06      2.710      0.007    5.41e-06     3.4e-05\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           0.0005      0.000      1.957      0.051   -2.56e-06       0.001\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                     -0.0013      0.000     -2.740      0.006      -0.002      -0.000\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    0.0841      0.040      2.125      0.034       0.006       0.162\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                            2.379e-05   7.85e-06      3.030      0.003    8.34e-06    3.92e-05\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          2.864e-05   7.73e-05      0.370      0.711      -0.000       0.000\n",
       "Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                           1.284e-05   7.46e-06      1.721      0.086   -1.83e-06    2.75e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0008      0.000     -2.085      0.038      -0.002   -4.68e-05\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    -2.53e-07    1.1e-07     -2.292      0.023    -4.7e-07   -3.59e-08\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\") -1.425e-06   1.14e-06     -1.249      0.212   -3.67e-06    8.19e-07\n",
       "==============================================================================\n",
       "Omnibus:                      214.307   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2354.664\n",
       "Skew:                           2.026   Prob(JB):                         0.00\n",
       "Kurtosis:                      14.174   Cond. No.                     1.20e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.2e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2055a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n"
     ]
    }
   ],
   "source": [
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c935e163",
   "metadata": {},
   "source": [
    "The code provided here aims to explore the concepts of in-sample and out-of-sample model performance metrics in linear regression, specifically focusing on the R-squared value. The code is structured to compare two models, Model 3 and Model 4, on the Pokémon dataset (pokeaman), which predicts the HP (health points) of a Pokémon based on various features. Here’s a step-by-step explanation of the code cells and what each one demonstrates:\n",
    "\n",
    "### 1. Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e4976e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "np.random.seed(130)\n",
    "pokeaman_train, pokeaman_test = train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14689ccb",
   "metadata": {},
   "source": [
    "This cell sets up the dataset by creating a 50-50 split into training and testing sets (`pokeaman_train` and `pokeaman_test`) using `train_test_split`. The split ensures that only half of the data is used to fit the model (in-sample), and the remaining half is used to assess model generalizability (out-of-sample).\n",
    "\n",
    "### 2. Building Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beda3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n"
     ]
    }
   ],
   "source": [
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()\n",
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model3)[0,1]**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad8d9fc",
   "metadata": {},
   "source": [
    "This cell defines and fits **Model 3**, which is a simple linear regression using only the `Attack` and `Defense` features to predict `HP`. After fitting the model to the training data, the code outputs the \"in-sample\" R-squared value, which represents how well the model explains the variance of `HP` within the training set.\n",
    "\n",
    "The \"out-of-sample\" R-squared is then calculated by predicting `HP` on the test set (`pokeaman_test`) and comparing these predictions (`yhat_model3`) to the actual `HP` values in the test set (`y`). This value indicates how well Model 3 generalizes to new data.\n",
    "\n",
    "### 3. Model 4 Setup and Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a94a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f808eeb",
   "metadata": {},
   "source": [
    "Here, **Model 4** is set up with a more complex formula that includes interaction terms across multiple features (`Attack`, `Defense`, `Speed`, `Legendary`, `Sp. Def`, and `Sp. Atk`). The `*` symbol represents interaction terms, which means Model 4 is designed to capture more complex relationships in the data. However, the cell limits the scope to avoid an excessive number of interactions that could lead to computational issues and overfitting.\n",
    "\n",
    "### 4. Building Model 4 and Calculating R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4c8a529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n"
     ]
    }
   ],
   "source": [
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()\n",
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y, yhat_model4)[0,1]**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aaf081",
   "metadata": {},
   "source": [
    "This cell fits **Model 4** on the training data and outputs both the in-sample and out-of-sample R-squared values. Model 4, due to its complexity, is expected to achieve a higher in-sample R-squared, as it can capture more variance in HP from the training data. However, this also makes it more susceptible to **overfitting**, meaning it may perform poorly on new, unseen data, indicated by a lower out-of-sample R-squared.\n",
    "\n",
    "### Explanation of Model Generalizability\n",
    "- **In-sample R-squared**: Calculated on the training data, showing how well the model fits the data it was trained on. Higher values here indicate better fit to the specific dataset used for training, but they don’t necessarily reflect performance on new data.\n",
    "- **Out-of-sample R-squared**: Calculated on the test data, representing the model’s ability to generalize. Lower values here suggest the model may be overfitting the training data and might not be capturing a general pattern that applies to other data.\n",
    "\n",
    "### Key Takeaways\n",
    "- **Model 3**: If Model 3 has relatively similar in-sample and out-of-sample R-squared values, it suggests a reasonable balance between complexity and generalizability.\n",
    "- **Model 4**: A higher in-sample R-squared combined with a substantially lower out-of-sample R-squared would indicate that Model 4 is likely overfitting—explaining the training data well but struggling to generalize to new data.\n",
    "\n",
    "This code illustrates the importance of comparing in-sample and out-of-sample performance to evaluate whether a model might be overfit. Models with high in-sample R-squared but low out-of-sample R-squared likely overfit the training data, meaning they capture noise rather than true patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac581d9",
   "metadata": {},
   "source": [
    "**[Link to the ChatBot Session](https://chatgpt.com/share/67324d57-bfd0-800e-80d0-0469870fd4a8)**\n",
    "\n",
    "**Summary of the ChatBot Session**\n",
    "In this session, we discussed the code that analyzes the in-sample and out-of-sample performance of two linear regression models using R-squared as a metric. Here’s a summary of the exchanges:\n",
    "\n",
    "Objective: We aimed to understand how well each model generalizes by comparing their R-squared values on training (in-sample) and test (out-of-sample) data.\n",
    "\n",
    "Dataset Splitting: We reviewed the dataset split, where the Pokémon dataset was divided into training (50%) and testing (50%) sets to evaluate model performance on unseen data.\n",
    "\n",
    "Model 3 Analysis: Model 3 was built using Attack and Defense as predictors for HP. The in-sample R-squared was calculated on the training data, and the out-of-sample R-squared was calculated on the test data. These R-squared values were compared to assess generalizability.\n",
    "\n",
    "Model 4 Setup and Analysis: Model 4 was a more complex model incorporating multiple features and interaction terms. We noted that while Model 4 might achieve a higher in-sample R-squared due to its complexity, it risks overfitting.\n",
    "\n",
    "Key Insights: We concluded that if the out-of-sample R-squared is significantly lower than the in-sample R-squared (especially in Model 4), the model is likely overfitting, suggesting it captures noise rather than generalizable patterns. The comparison demonstrates the importance of using different training and testing datasets to confirm a model's predictive validity on unseen data.\n",
    "\n",
    "This discussion highlighted the value of in-sample versus out-of-sample comparisons to assess a model’s ability to generalize and avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78558a3d",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d8386b",
   "metadata": {},
   "source": [
    "The `model4_linear_form` in this context defines an overly complex regression model by introducing numerous interactions and terms among predictor variables, resulting in high multicollinearity within the design matrix (`model4_spec.exog`). The \"design matrix\" is a matrix where each column represents a different predictor variable or interaction term in the model. High multicollinearity, seen in a high correlation among columns in `model4_spec.exog` (checked with np.corrcoef(`model4_spec.exog`)), inflates the condition number and indicates that some predictors are nearly linearly dependent. This linear dependence makes it hard for the model to separate true predictive relationships from random noise specific to the training dataset.\n",
    "\n",
    "Because of this high multicollinearity, the model is prone to overfitting. In this case, model4 is so complex relative to the training data's size and information content that it \"learns\" accidental patterns or noise in the training set. As a result, predictions made on new, unseen data (i.e., \"out of sample\") do not generalize well. This lack of generalizability stems from the model capturing random correlations that do not reflect underlying patterns in the population, leading to poor predictive performance outside the training data.\n",
    "\n",
    "In contrast, simpler models, such as model3, avoid excessive complexity and focus on fewer, more reliable predictor relationships. This parsimony allows model3 to identify patterns in the training data that also hold in new data, making it more generalizable. Hence, while multicollinearity among predictors can destabilize estimates and harm generalization, centering and scaling can sometimes reduce collinearity. Nonetheless, a very high condition number for model4_fit, even after centering and scaling, signals substantial multicollinearity issues that undermine its reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb56d44",
   "metadata": {},
   "source": [
    "**[Link to the ChatBot Session](https://chatgpt.com/share/67324e1f-6040-800e-a09d-a459a1cd0c51)**\n",
    "\n",
    "**Summary of the ChatBot Session**\n",
    "\n",
    "Summary of Chatbot Interactions\n",
    "\n",
    "Understanding Model4 Linear Form and Design Matrix\n",
    "We discussed how model4_linear_form defines a complex regression model by creating numerous interaction terms and predictors in the design matrix (model4_spec.exog). This complexity introduces high multicollinearity, observed via correlation among predictors (np.corrcoef(model4_spec.exog)), which inflates the condition number and reveals nearly linear dependencies among predictors.\n",
    "\n",
    "Impact of Multicollinearity on Model Generalization\n",
    "High multicollinearity leads to an overfitted model, where model4 captures noise specific to the training dataset instead of generalizable patterns. As a result, predictions made by model4 do not generalize well to new data, causing poor out-of-sample performance. This overfitting issue arises because model4's complexity exceeds the amount of reliable information in the training dataset.\n",
    "\n",
    "Comparison with Simpler Model3\n",
    "We contrasted this with model3, a simpler model that avoids excessive complexity and identifies stable, predictive associations. Model3's parsimony allows it to capture genuine patterns that are likely to generalize to new data, making it more reliable.\n",
    "\n",
    "Condition Number and Best Practices for Multicollinearity\n",
    "We discussed that the condition number provides a measure of multicollinearity, where a high value signals concerns about model reliability. While centering and scaling predictors can sometimes reduce multicollinearity, the extremely high condition number for model4_fit—despite centering and scaling—underscores its excessive complexity and unreliability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba868b4",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d851bf",
   "metadata": {},
   "source": [
    "The progression from `model3_fit` and `model4_fit` to `model5_linear_form`, `model6_linear_form`, and `model7_linear_form` reflects a step-by-step approach in enhancing model complexity based on hypothesis testing and generalizability metrics. Each model builds on its predecessor by selectively adding or adjusting predictors to improve performance while assessing multicollinearity and prediction accuracy, both in-sample and out-of-sample.\n",
    "\n",
    "For instance:\n",
    "\n",
    "- **Model 5** incorporates core predictors (like `Attack`, `Defense`, and `Legendary` status) and categorical predictors (e.g., `Generation`, `Type 1`, and `Type 2`) based on prior evidence of their predictive value.\n",
    "- **Model 6** refines this by narrowing down to significant predictors from Model 5, retaining only those that are statistically meaningful.\n",
    "- **Model 7** further extends complexity by introducing interactions among the quantitative predictors (`Attack`, `Speed`, `Sp. Def`, and `Sp. Atk`), capturing their combined effect on `HP`.\n",
    "\n",
    "The introduction of \"centered and scaled\" variables in `model7_linear_form_CS` addresses multicollinearity by reducing the condition number (from a very high 2.34 billion down to 15.4), which, while notable, is not a concern under common thresholds for multicollinearity (<30). This scaling adjustment allows for clearer interpretation of predictor contributions, minimizing distortions due to correlated predictors while preserving model generalizability.\n",
    "\n",
    "In summary, the evolution of these models follows an evidence-based approach to maximize predictive accuracy, balance model complexity, and mitigate multicollinearity effects through strategic variable inclusion, interaction terms, and centering/scaling transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a1e83b",
   "metadata": {},
   "source": [
    "**[Link to the ChatBot Session](https://chatgpt.com/share/67324f69-0a34-800e-a74b-f486cbe4d6e5)**\n",
    "\n",
    "**Summary of the ChatBot Session**\n",
    "Certainly! Here’s a summary of our interaction in this session:\n",
    "\n",
    "You asked for guidance on the rationale and principles behind the stepwise development of regression models (model5_linear_form through model7_linear_form) from earlier models (model3_fit and model4_fit), specifically in the context of evidence-based modeling and multicollinearity management.\n",
    "We discussed how each model version builds upon its predecessor by incrementally adding, refining, or removing predictors to improve predictive performance and ensure model generalizability. This involves balancing complexity with interpretability and ensuring predictive associations are well-supported by the data.\n",
    "For instance, model5 includes various predictors based on prior analysis, model6 narrows down to significant predictors, and model7 adds interactions to capture combined effects on the outcome (HP).\n",
    "We also examined how centering and scaling in model7_CS reduces multicollinearity, as indicated by the lower condition number (15.4) when compared to the unscaled version. This adjustment supports model stability and interpretability by minimizing multicollinearity, allowing clearer attribution of predictor contributions.\n",
    "I provided a concise summary emphasizing that the evolution of these models follows an evidence-based approach, with the goal of maximizing predictive power while addressing multicollinearity and balancing model complexity.\n",
    "This summary captures the development of the models in terms of evidence-based improvements, generalizability, and techniques to manage multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f0fd43",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a976de5",
   "metadata": {},
   "source": [
    "The code snippet provided illustrates a loop that repeatedly trains a linear regression model on different train-test splits of the data, collecting both \"in-sample\" (training set) and \"out-of-sample\" (testing set) $R^2$ values. This repeated sampling approach highlights how model performance can vary between different data splits and helps illustrate potential overfitting or instability in the model's generalizability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d60c3d",
   "metadata": {},
   "source": [
    "### Explanation of Key Parts in the Code\n",
    "**1. Train-Test Splits**: Each iteration of the loop splits the data into training (31 samples) and testing sets. By changing the split each time, we observe how the model's $R^2$ scores differ across various subsets of the data, providing insight into the model's stability and robustness.\n",
    "\n",
    "**2. Model Training and Prediction**: In each iteration, a linear regression model is fitted to the training data based on the specified formula (danceability ~ energy * loudness + energy * mode). The model’s $R^2$ score on the training data is recorded as the \"in-sample\" performance.\n",
    "\n",
    "**3. Out-of-Sample Performance**: The $R^2$ value for the model on the testing data (out-of-sample) is also computed by correlating the predicted values with the actual test set values. This gives an idea of how well the model generalizes to new data, which it wasn’t trained on.\n",
    "\n",
    "**4. Visualization**: After 100 iterations, the collected in-sample and out-of-sample $R^2$ values are plotted against each other. A reference line y=x is added to help visually assess when in-sample performance is better than out-of-sample performance (above the line) or vice versa (below the line)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe335dde",
   "metadata": {},
   "source": [
    "### Purpose of the Demonstration and Interpretation of Results\n",
    "This demonstration aims to show the variance in model performance when the data is split in different ways and to illustrate potential issues such as overfitting or variability in the model's stability:\n",
    "\n",
    "- **Overfitting**: If the model achieves high $R^2$ values in-sample but lower values out-of-sample, this suggests overfitting. The model is capturing patterns specific to the training set that don’t generalize well to new data.\n",
    "\n",
    "- **Variance in Model Performance**: Variations in both in-sample and out-of-sample performance can indicate instability in the model. A highly variable out-of-sample performance across different data splits implies that the model may not be robust and could lead to unreliable predictions.\n",
    "\n",
    "- **Occasional Higher Out-of-Sample Performance**: The situation where out-of-sample $R^2$ is occasionally higher than in-sample $R^2$, as observed with the initial split using np.random.seed(130), does not necessarily imply underfitting. Instead, it reflects the randomness in train-test splits, where certain splits may better align with the model's form or the specific data configuration. This randomness can lead to occasional cases where out-of-sample performance is better due to the unique characteristics of that particular split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f49c295",
   "metadata": {},
   "source": [
    "### Summary of Results and Significance\n",
    "The key takeaway from this exercise is that the performance metrics of a model can fluctuate based on the specific data split. By visualizing in-sample vs. out-of-sample $R^2$ scores across numerous splits, we can better understand the model's consistency and the risk of overfitting. A robust model should ideally exhibit similar performance in both training and testing, indicating generalizability across unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968df7a0",
   "metadata": {},
   "source": [
    "**[Link to the ChatBot Session](https://chatgpt.com/share/67325111-c288-800e-8f9a-c04f83997a6c)**\n",
    "\n",
    "**Summary of the ChatBot Session**\n",
    "\n",
    "Here’s a summary of our exchanges during this chatbot session:\n",
    "\n",
    "Task Assistance: You requested help with a coding task involving repeated train-test splits for model evaluation. Specifically, the goal was to create a for loop that collects and visualizes in-sample and out-of-sample \n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    "  values from a linear regression model. I explained the purpose of the loop and the potential implications of the results, including overfitting and model stability. I also clarified the significance of out-of-sample performance occasionally exceeding in-sample performance.\n",
    "\n",
    "Markdown Formatting: You asked how to format \n",
    "𝑅\n",
    "2\n",
    "R \n",
    "2\n",
    "  in Markdown. I provided guidance on how to use LaTeX math notation ($R^2$ for inline and $$R^2$$ for block formatting) and noted that plain text (R^2) can also be used when LaTeX is not supported.\n",
    "\n",
    "Let me know if you need further clarification or additional details!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89286df3",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e743cc6",
   "metadata": {},
   "source": [
    "The illustration in the text compares two linear regression models, `model7_fit` and `model6_fit`, in terms of complexity, interpretability, and generalizability. Here's a breakdown of the key points:\n",
    "\n",
    "**1. Model Complexity**:\n",
    "- `model7_fit` is more complex than `model6_fit`, including higher-order interactions and additional terms.\n",
    "- The complexity of `model7_fit` might lead to overfitting, where the model captures patterns that are specific to the training data but don't generalize well to new data.\n",
    "\n",
    "**2. Evidence from Coefficients**:\n",
    "- The evidence for the coefficients in `model7_fit` is weaker, as indicated by its p-values, compared to `model6_fit`, where the coefficients have stronger evidence in the data.\n",
    "- This suggests that while `model7_fit` may fit the training data better, its findings may not be as reliable.\n",
    "\n",
    "**3. Model Parsimony and Interpretability**:\n",
    "- Simpler models like `model6_fit` are easier to interpret and provide clearer insights, especially when it comes to understanding the relationships between variables.\n",
    "- The complexity of `model7_fit`, with terms like a four-way interaction variable, makes interpretation more challenging.\n",
    "\n",
    "**4. Out-of-Sample Performance**:\n",
    "- `model7_fit` performs better out-of-sample (on unseen data) than `model6_fit`. However, this improved performance might not justify the added complexity.\n",
    "- The text emphasizes that model interpretability and generalizability are often more important than raw performance, especially when the performance difference is not significant.\n",
    "\n",
    "**5. Generalizability Concerns**:\n",
    "- The code demonstrates an additional test where data is divided by \"Generations,\" simulating a more realistic situation where data arrives sequentially over time.\n",
    "- When applying the model to future data (not seen during training), both models show generalizability issues, but these are more pronounced for `model7_fit` due to its complexity.\n",
    "\n",
    "**6. Final Takeaway**:\n",
    "- The primary message is that simpler models (like `model6_fit`) often offer more reliable generalizability and easier interpretability, making them preferable in many situations, especially when the performance difference with a more complex model is marginal.\n",
    "- Complex models should only be used when they clearly outperform simpler models, as they come with the risk of overfitting and decreased interpretability.\n",
    "\n",
    "In summary, this illustration emphasizes the trade-off between model complexity, performance, and interpretability. While a more complex model might perform better in terms of raw metrics, a simpler model is often preferable due to its better generalizability and ease of understanding, especially when both models perform similarly on out-of-sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9536ed9",
   "metadata": {},
   "source": [
    "**Extra**: \n",
    "\n",
    "**1. Model Complexity**:\n",
    "\n",
    "- Model7_fit has better \"out of sample\" prediction performance than model6_fit but is more complex. Its complexity is evident in its inclusion of high-order interactions, such as a four-way interaction among variables (e.g., Attack, Speed, Sp. Def, and Sp. Atk).\n",
    "- Model6_fit is simpler, with fewer interactions and coefficients, making it easier to interpret and less prone to picking up random patterns specific to the training data (overfitting).\n",
    "\n",
    "**2. Interpretability vs. Performance**:\n",
    "\n",
    "- Despite model7_fit’s better predictive power, model6_fit might be preferred for its interpretability. High complexity in model7_fit makes it difficult to understand, especially with interactions that are challenging to interpret in practical terms.\n",
    "- Often, simplicity (parsimony) in models allows for clearer insights and better generalizability, which is essential when model interpretability is prioritized over marginal gains in predictive accuracy.\n",
    "\n",
    "**3. Generalizability Concerns**:\n",
    "\n",
    "- Generalizability is about how well a model trained on one dataset (or a subset of data) can predict future or unseen data. Using the training data’s idiosyncratic patterns, as model7_fit might do, can lead to overfitting.\n",
    "- In this example, a simulation mimics how the models perform when the data is used in a real-world setting where predictions are made for future data based on patterns in older data.\n",
    "- The results show that model7_fit, with its complexity, struggles more with generalizability compared to model6_fit, especially when the dataset includes changes over time (such as across \"Generations\").\n",
    "\n",
    "**4. Recommendation for Parsimony**:\n",
    "\n",
    "- A simpler model like model6_fit is recommended unless a complex model clearly and significantly outperforms it. Simpler models are often more reliable and easier to understand and interpret.\n",
    "\n",
    "\n",
    "The code provided illustrates these points by calculating \"in-sample\" and \"out-of-sample\" R-squared values for each model when trained and tested across different \"Generations\" of data. Model7_fit’s performance varies more significantly, revealing generalizability issues, while model6_fit maintains relatively stable performance, supporting the preference for simplicity and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c62865",
   "metadata": {},
   "source": [
    "**[Link to the ChatBot Session](https://chatgpt.com/share/67325227-0514-800e-b30d-29ece7e98591)**\n",
    "\n",
    "**[Link to the Extra part ChatBot Session](https://chatgpt.com/share/67342a5a-8de8-800e-9091-e5814d241635)**\n",
    "\n",
    "**Summary of the ChatBot Session**\n",
    "\n",
    "Here’s a summary of our exchanges in this chatbot session:\n",
    "\n",
    "Explanation Request: You asked for an explanation of a complex comparison between two models (model7_fit and model6_fit) that were part of a larger discussion about linear regression models. You were seeking clarification on the trade-off between model complexity, performance, and interpretability, as well as the potential for overfitting in more complex models.\n",
    "\n",
    "Guidance on Model Comparison: The text you provided discussed the differences between the two models, highlighting how model7_fit is more complex but less interpretable and harder to generalize. It was explained that while model7_fit showed better performance on test data, model6_fit, being simpler, had more consistent generalizability and stronger coefficient evidence. The importance of model simplicity and interpretability was emphasized, especially when the performance difference between models is minimal.\n",
    "\n",
    "Code and Generalizability: The provided code was intended to further illustrate the potential generalizability concerns when applying these models to new data (based on \"Generations\"), simulating a real-world scenario where data arrives sequentially. The code demonstrated that while both models had some issues with generalizability, the simpler model6_fit seemed to handle future data better than model7_fit.\n",
    "\n",
    "Main Takeaways: The main takeaway from the discussion was that simpler models tend to be more interpretable and generalizable, and they should be preferred unless a more complex model can consistently outperform them.\n",
    "\n",
    "This session provided a deeper understanding of model selection in regression analysis, with a focus on balancing complexity and interpretability for reliable and generalizable predictions.\n",
    "\n",
    "**Extra Summary**\n",
    "Initial Question: You asked for help in understanding and explaining the complexity, interpretability, and generalizability of two models, model6_fit and model7_fit, as described in an assignment prompt. The focus was on comparing these models in terms of their ability to generalize and the importance of choosing simpler, interpretable models when model performance is relatively similar.\n",
    "\n",
    "Explanation Provided: I summarized the key points from the prompt, including:\n",
    "\n",
    "Model Complexity: Explained how model7_fit is more complex, includes higher-order interactions, and tends to overfit compared to model6_fit.\n",
    "Interpretability vs. Performance: Highlighted the value of simpler models (like model6_fit) for better interpretability, especially when performance differences are minor.\n",
    "Generalizability Concerns: Addressed how the complexity of model7_fit can impact its performance on unseen data, particularly across different \"Generations\" in a real-world scenario.\n",
    "Recommendation for Parsimony: Emphasized that simpler models are often more reliable and easier to understand and interpret.\n",
    "Code Explanation: Provided an overview of the code, which demonstrated the models’ \"in-sample\" and \"out-of-sample\" R-squared values across different data splits, showing model6_fit’s better generalizability and more stable performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
